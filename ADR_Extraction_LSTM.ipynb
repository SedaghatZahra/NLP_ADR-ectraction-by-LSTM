{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Library modules\n",
    "\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import *\n",
    "import random\n",
    "from colorama import Fore, Back, Style\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score ,accuracy_score,classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import warnings\n",
    "import unicodedata\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Classes & Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the specific format for saving the read data \n",
    "\n",
    "class TextGetter(object):\n",
    "    \n",
    "    def __init__(self,groupName,data):\n",
    "        self.data = data\n",
    "        self.groupName = groupName\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t,Id,Po,Se) for w, t , Id, Po, Se in zip(s[\"Word\"].values.tolist(),\n",
    "                                                     s[\"Tag\"].values.tolist(),\n",
    "                                                     s[\"Id\"].values.tolist(),\n",
    "                                                     s[\"Post #\"].values.tolist(),s[\"Sentence #\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(self.groupName,sort=False).apply(agg_func)\n",
    "        self.texts = [s for s in self.grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_summary(tag,B_NextI): \n",
    "    pred = ['Sum']*len(tag)\n",
    "    y_actu = pd.Series(tag, name='Label')\n",
    "    y_pred = pd.Series(pred, name='')\n",
    "    df= pd.crosstab(y_actu, y_pred) \n",
    "    df = df.reset_index()\n",
    "            \n",
    "    if B_NextI == True:    \n",
    "        df['Row'] = df['Label'].copy()\n",
    "        for ii in range(len(df['Row'])):\n",
    "            if (df['Row'][ii][:2] == 'B-') or (df['Row'][ii][:2] == 'I-'): \n",
    "                df['Row'][ii] = df['Row'][ii][2:]    \n",
    "        for index in range(len(df)):\n",
    "            if df.loc[index,'Row']=='O':\n",
    "                df.loc[index,'Row']='zzzz'    \n",
    "        \n",
    "        df = df.sort_values(by = ['Row','Label'],axis=0)\n",
    "        df = df.drop('Row',axis= 1)\n",
    "    else:    \n",
    "        for index in range(len(df)):\n",
    "            if df.loc[index,'Label']=='O':\n",
    "                df.loc[index,'Label']='zzzz' \n",
    "                \n",
    "        df = df.sort_values(by = ['Label'],axis=0)\n",
    "        \n",
    "        for index in range(len(df)):\n",
    "            if df.loc[index,'Label']=='zzzz':\n",
    "                df.loc[index,'Label']='O' \n",
    "    \n",
    "    Column_List = df.columns.to_list()\n",
    "    Column_List_New = ['Label']\n",
    "    for lab in TagLabel :\n",
    "        if lab in Column_List:\n",
    "            Column_List_New.extend([lab])\n",
    "    if len(Column_List_New)==1:\n",
    "        Column_List_New = Column_List_New+ ['Sum']\n",
    "       \n",
    "    if B_NextI == True:        \n",
    "        Column_List_New = ['Label']\n",
    "        for lab in TagLabel :\n",
    "            if lab in Column_List:\n",
    "                Column_List_New.extend([lab])\n",
    "        if len(Column_List_New)==1:\n",
    "            Column_List_New = Column_List_New+ ['Sum']\n",
    "        Column_List = Column_List_New\n",
    "          \n",
    "    df = df.reindex( columns=Column_List)          \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for making confusion matrix with and without \"O\" label\n",
    "\n",
    "def Make_Confusion_Graph_Tabel(target,pred,Labels,Prefix,Font_Scale,ShowPercent,Sum_O_Zero): \n",
    "\n",
    "    if ShowPercent:\n",
    "        aa = confusion_matrix(target,pred,labels= Labels,normalize= 'true')\n",
    "        aa = 100.0 * aa\n",
    "        Fmt = '.1f'        \n",
    "    else:\n",
    "        aa = confusion_matrix(target,pred,labels= Labels)\n",
    "        Fmt = 'd'\n",
    "        \n",
    "    if Sum_O_Zero:\n",
    "        aa[len(aa[:,0])-1,len(aa[:,0])-1] = 0\n",
    "        \n",
    "    if Prefix:                          \n",
    "        df_cm = pd.DataFrame(aa, index = [i for i in Labels], columns = [i for i in Labels]) \n",
    "    else:\n",
    "        df_cm = pd.DataFrame(aa, index = [i[2:] if i != 'O' else i for i in Labels], columns = [i[2:] if i != 'O' else i for i in Labels ]) \n",
    "\n",
    "    FigSize =(len(Labels)* 0.8 ,len(Labels)*0.4)\n",
    "    plt.figure(figsize = FigSize)\n",
    "    sn.set(font_scale=Font_Scale) # for label size\n",
    "    sn.heatmap(df_cm, annot=True,cmap='Blues', cbar=False, annot_kws={\"size\": 14},fmt = Fmt)\n",
    "    plt.show()\n",
    "    return df_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the input reviews and organize their proper labels (assign \"O\" to not important tokens)\n",
    "\n",
    "def tokenize_and_preserve_labels(getter_Texts):\n",
    "    Tokenized_Texts =[[]]*len(getter_Texts)\n",
    "    Labels =[[]]*len(getter_Texts)\n",
    "    \n",
    "    for ii,sent in enumerate(getter_Texts):\n",
    "        Tokenized_Texts[ii] =['[CLS]']\n",
    "        Labels[ii] =['O']\n",
    "        token =[]\n",
    "        lab = []\n",
    "        for s in sent:\n",
    "            tokenized_s = tokenizer.tokenize(s[0])\n",
    "            labe_s = s[1]\n",
    "            label =[] \n",
    "            label = [labe_s]*len(tokenized_s)\n",
    "            token.extend(tokenized_s)        \n",
    "            lab.extend(label)    \n",
    "        Tokenized_Texts[ii].extend(token)\n",
    "        Tokenized_Texts[ii].extend(['[SEP]'])\n",
    "        \n",
    "        Labels[ii].extend(lab)\n",
    "        Labels[ii].extend(['O'])\n",
    "    return Tokenized_Texts,Labels           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tokenize and their related labels to index\n",
    "\n",
    "def ids_and_mask(text_tokenize,Labels):\n",
    "    text_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in text_tokenize],value=0,\n",
    "                          maxlen=seq_Length, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in Labels], maxlen=seq_Length,\n",
    "                           value=-100, dtype=\"long\", truncating=\"post\", padding=\"post\") \n",
    "                          \n",
    "    text_mask = [[int(i>0) for i in ii] for ii in text_ids]\n",
    "    \n",
    "    return text_ids,tags,text_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the loss and accuracy values\n",
    "\n",
    "def plot_loss_accuracy(df_score,xlim_from,xlim_to,yLow_loss,yHigh_loss,yLow_accuracy,flag_title):\n",
    "    sn.set(context=\"notebook\", style=\"white\", palette=None,\n",
    "               font=\"sans-serif\", font_scale=1.1, color_codes=True, rc=None)\n",
    "    df_plot = df_score.copy()\n",
    "    plt.figure(figsize = (16,4))\n",
    "\n",
    "    # Plot negative loss function\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.xlim(xlim_from,xlim_to)\n",
    "    \n",
    "    \n",
    "    df_plot.loc[0,'Tr_loss']=1000\n",
    "    df_plot.loc[0,'Va_loss']=1000\n",
    "\n",
    "    \n",
    "    Tr_loss, = plt.plot(df_plot.Tr_loss)\n",
    "    Va_loss, = plt.plot(df_plot.Va_loss)\n",
    "    \n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Loos')\n",
    "    plt.yscale('log')\n",
    "    plt.ylim(yLow_loss,yHigh_loss)\n",
    "    plt.grid(ls='--')\n",
    "    if flag_title == 1:\n",
    "        plt.title('Loss - train ends at %.2f | val at %.2f' %(df_plot.tail(1).Tr_loss, df_plot.tail(1).Va_loss))\n",
    "    else:\n",
    "        plt.title('Loss (Training & Validation)')\n",
    "        \n",
    "    val_xmin_loss = np.argmin(np.array(df_plot.Va_loss)) \n",
    "    val_ymin_loss = min(df_plot.Va_loss)\n",
    "    aaa = plt.scatter(val_xmin_loss, val_ymin_loss, marker='o', color='black',lw=2) # plot point    \n",
    "    \n",
    "    plt.legend([Tr_loss, Va_loss,aaa], ['Training', 'Validation','Valid Min'])\n",
    "    \n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.xlim(xlim_from,xlim_to)\n",
    "    Tr_accuracy, = plt.plot(df_plot.Tr_accur)\n",
    "    Va_accuracy, = plt.plot(df_plot.Va_accur)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy (%)')\n",
    "    plt.ylim(yLow_accuracy,101)\n",
    "    plt.grid(ls='--')\n",
    "    if flag_title == 1:\n",
    "        plt.title('Accuracy - train ends at %.2f%% | val at %.2f%%' %(df_plot.tail(1).Tr_accur , df_plot.tail(1).Va_accur ))\n",
    "    else:\n",
    "        plt.title('Accuracy (Training & Validation)')\n",
    "\n",
    "    val_xmax_accu = np.argmax(np.array(df_plot.Va_accur))    \n",
    "    val_ymax_accu = max(df_plot.Va_accur)\n",
    "    \n",
    "    aaa = plt.scatter(val_xmax_accu, val_ymax_accu, marker='o', color='black',lw=2) # plot point    \n",
    "    plt.legend([Tr_accuracy, Va_accuracy,aaa], ['Training', 'Validation','Valid Max'])\n",
    "    global fig\n",
    "    fig = plt.gcf()\n",
    "    plt.show(block= False)\n",
    "    \n",
    "    if flag_title == 1:\n",
    "        print(\"Min Valid Loss: %.4f     in Epoch: %.f\" %(val_ymin_loss,val_xmin_loss))\n",
    "        print(\"Max Valid Accu: %.2f %%    in Epoch: %.f\" %(val_ymax_accu,val_xmax_accu)) \n",
    "        print(\"____________________________________________________________\")\n",
    "    return (True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the F1_score value\n",
    "\n",
    "def plot_F1_Score(df_score,xlim_from,xlim_to,yLow_F1,flag_title): \n",
    "    sn.set(context=\"notebook\", style=\"white\", palette=None,\n",
    "               font=\"sans-serif\", font_scale=1.1, color_codes=True, rc=None)\n",
    "    plt.figure(figsize = (16,4))\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.xlim(xlim_from,xlim_to)\n",
    "    train_F1Score, = plt.plot(df_score.Tr_F1)\n",
    "    valid_F1Score, = plt.plot(df_score.Va_F1)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('f1-score')\n",
    "    plt.ylim(yLow_F1,1.05)\n",
    "    plt.grid(ls='--')\n",
    "    if flag_title == 1:\n",
    "        plt.title('f1-score - train ends at %.2f | val at %.2f' %(df_score.tail(1).Tr_F1 , df_score.tail(1).Va_F1 ))\n",
    "    else:\n",
    "        plt.title('f1-score (Training & Validation)')\n",
    "        \n",
    "        \n",
    "    valid_xmax_F1 = np.argmax(np.array(df_score.Va_F1))          \n",
    "    valid_ymax_F1 = max(df_score.Va_F1)\n",
    "    aaa = plt.scatter(valid_xmax_F1, valid_ymax_F1, marker='o', color='black',lw=2) # plot point    \n",
    "    plt.legend([train_F1Score, valid_F1Score,aaa], ['Training', 'Validation','Valid Max'])     \n",
    "              \n",
    "    train_ymax_F1 = max(df_score.Tr_F1)\n",
    "    train_xmax_F1 = np.argmax(np.array(df_score.Tr_F1))    \n",
    "    \n",
    "    global fig\n",
    "    fig = plt.gcf()\n",
    "    plt.show(block= False)\n",
    "    if flag_title == 1:\n",
    "        print(\"Training   Max f1-score: %.4f     in Epoch: %.f\" %(train_ymax_F1,train_xmax_F1))\n",
    "        print(\"Validation Max f1-score: %.4f     in Epoch: %.f\" %(valid_ymax_F1,valid_xmax_F1))\n",
    "    print(\"____________________________________________________________\")\n",
    "    return (True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the BiLSTM model\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, lstm_layers,\n",
    "               emb_dropout, lstm_dropout, fc_dropout, word_pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # LAYER 1: Embedding\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=input_dim, \n",
    "            embedding_dim=embedding_dim, \n",
    "            padding_idx=word_pad_idx\n",
    "        )\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "        # LAYER 2: BiLSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            bidirectional=True,\n",
    "            dropout=lstm_dropout if lstm_layers > 1 else 0\n",
    "        )\n",
    "        # LAYER 3: Fully-connected\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # times 2 for bidirectional\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embedding_out = self.emb_dropout(self.embedding(sentence))\n",
    "        lstm_out, _ = self.lstm(embedding_out)\n",
    "        out = self.fc(self.fc_dropout(lstm_out))\n",
    "        return out\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            nn.init.normal_(param.data, mean=0, std=0.1)\n",
    "            \n",
    "    def init_embeddings(self, word_pad_idx):\n",
    "        self.embedding.weight.data[word_pad_idx] = torch.zeros(self.embedding_dim)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End (Defining Classes & Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describing and initializing the random seed (the random seed is used to get the same random number in each \n",
    "# repetition of codes)\n",
    "\n",
    "seed = 3054\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# estefadeh az GPU dara soorate vojod\n",
    "# n_gpu: 0  used \"CPU\"   &  n_gpu: 1  used \"GPU\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.device(\"cuda\") \n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    print(\"torch.device : cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:    \n",
    "    torch.device(\"cpu\")\n",
    "    n_gpu = 0;\n",
    "    print(\"torch.device : cpu\")\n",
    "print(n_gpu)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reading and loading the dataset6\n",
    "\n",
    "data = pd.read_csv(\"./Data/Dokhtara/Dokhtara_Word_Dataset_Main.csv\", encoding=\"utf-8\").fillna(method=\"ffill\")\n",
    "getterTexts = TextGetter('Post #',data).texts\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the percentage of splitting the dataset\n",
    "\n",
    "valid_size_real = 0.15\n",
    "test_size = 0.30\n",
    "valid_size = valid_size_real /(1- test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling and splitting the train, valid and test dataset\n",
    "\n",
    "valid_size_real = 0.15\n",
    "test_size = 0.30\n",
    "valid_size = valid_size_real /(1- test_size)\n",
    "Random_State = 2018\n",
    "\n",
    "print('number text:',len(getterTexts),'\\n')\n",
    "\n",
    "trainValid_getter, test_getter = train_test_split(getterTexts, random_state=Random_State, test_size=test_size)\n",
    "train_getter, valid_getter = train_test_split(trainValid_getter,random_state=Random_State, test_size=valid_size)\n",
    "\n",
    "print('number train       :',len(train_getter))\n",
    "print('number valid       :',len(valid_getter))\n",
    "print('number test        :',len(test_getter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# initializing some hyperparameters\n",
    "\n",
    "seq_Length = 100\n",
    "BatchSize = 32\n",
    "LSTM_N = 150\n",
    "\n",
    "LRate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning a unique index to each label\n",
    "\n",
    "tags_name = list(set(g[1] for gett in getterTexts for g in gett))\n",
    "tags_name.sort()\n",
    "tag2idx = {t: i for i, t in enumerate(tags_name)}\n",
    "num_labels = len(tags_name)\n",
    "\n",
    "TagLabel =tags_name.copy()\n",
    "TagLabel.remove('O')   # without \"O\"\n",
    "\n",
    "print(\"num_labels :\",num_labels)\n",
    "print(\"\")\n",
    "print(\"Tags name  :\",tags_name)\n",
    "print(\"\")\n",
    "print(\"tag2idx    :\",tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# a brief report of the distribution of the label in the dataset \n",
    "\n",
    "\n",
    "BNextI = True\n",
    "AllTags = [g[1] for gett in getterTexts for g in gett]\n",
    "report_dataSet = dataset_summary(AllTags, B_NextI= BNextI)\n",
    "\n",
    "TrTags = [g[1] for gett in train_getter for g in gett]\n",
    "report_Train = dataset_summary(TrTags, B_NextI= BNextI)\n",
    "\n",
    "VaTags = [g[1] for gett in valid_getter for g in gett]\n",
    "report_Valid = dataset_summary(VaTags, B_NextI= BNextI)\n",
    "\n",
    "TsTags = [g[1] for gett in test_getter for g in gett]\n",
    "report_Test = dataset_summary(TsTags, B_NextI= BNextI)\n",
    "\n",
    "res = pd.DataFrame({'Label':tags_name})\n",
    "res['Train'] = 0\n",
    "res['Valid'] = 0\n",
    "res['Test'] = 0\n",
    "res['Sum'] = [0]*len(res)\n",
    "for ii in range(len(res)):\n",
    "    \n",
    "    for jj in range(len(report_Train)):\n",
    "        if res['Label'][ii] == report_Train['Label'][jj]:\n",
    "            res['Train'][ii]  = report_Train['Sum'][jj]\n",
    "            break           \n",
    "    \n",
    "    for jj in range(len(report_Valid)):\n",
    "        if res['Label'][ii] == report_Valid['Label'][jj]:\n",
    "            res['Valid'][ii]  = report_Valid['Sum'][jj]\n",
    "            break   \n",
    "            \n",
    "    if len(test_getter) != 0.0 :\n",
    "        for jj in range(len(report_Test)):\n",
    "            if res['Label'][ii] == report_Test['Label'][jj]:\n",
    "                res['Test'][ii]  = report_Test['Sum'][jj]\n",
    "                break\n",
    "            \n",
    "    for jj in range(len(report_dataSet)):\n",
    "        if res['Label'][ii] == report_dataSet['Label'][jj]:\n",
    "            res['Sum'][ii]  = report_dataSet['Sum'][jj]\n",
    "            break\n",
    "            \n",
    "print('*** Data Set ***\\n\\n')\n",
    "print('train (num text)  :',len(train_getter))\n",
    "print('valid (num text)  :',len(valid_getter))\n",
    "print('test  (num text)  :',len(test_getter))\n",
    "print('')\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a vocabulary\n",
    "\n",
    "words = list(set(data[\"Word\"].values))\n",
    "\n",
    "words.append(\"XWords\")\n",
    "words.append(\"ENDPAD\")\n",
    "# Converting greek characters to ASCII characters eg. 'naïve café' to 'naive cafe' \n",
    "words = [unicodedata.normalize('NFKD', str(w)).encode() for w in words]\n",
    "n_words = len(words)\n",
    "print(\"Length of vocabulary = \",n_words)\n",
    "\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "\n",
    "PAD_value = word2idx[b'ENDPAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the tokens and their true labels to indices and padding the sequences with lengths less than seq_Length\n",
    "\n",
    "\n",
    "X_Train = [[word2idx[unicodedata.normalize('NFKD', str(w[0])).encode()] for w in c] for c in train_getter]\n",
    "train_ids = pad_sequences(maxlen=seq_Length, sequences=X_Train, padding=\"post\", value=n_words-1)\n",
    "\n",
    "\n",
    "y_Train = [[tag2idx[w[1]] for w in c] for c in train_getter]\n",
    "y_Train = pad_sequences(maxlen=seq_Length, sequences=y_Train, padding=\"post\", value=-100)\n",
    "tr_tags = y_Train.copy()\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "X_Valid = [[word2idx[unicodedata.normalize('NFKD', str(w[0])).encode()] for w in c] for c in valid_getter]\n",
    "valid_ids = pad_sequences(maxlen=seq_Length, sequences=X_Valid, padding=\"post\", value=n_words-1)\n",
    "\n",
    "y_Valid = [[tag2idx[w[1]] for w in c] for c in valid_getter]\n",
    "y_Valid = pad_sequences(maxlen=seq_Length, sequences=y_Valid, padding=\"post\", value=-100)\n",
    "val_tags = y_Valid.copy()\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "    \n",
    "X_Test = [[word2idx[unicodedata.normalize('NFKD', str(w[0])).encode()] for w in c] for c in test_getter]\n",
    "test_ids = pad_sequences(maxlen=seq_Length, sequences=X_Test, padding=\"post\", value=n_words-1)\n",
    "\n",
    "y_Test = [[tag2idx[w[1]] for w in c] for c in test_getter]\n",
    "y_Test = pad_sequences(maxlen=seq_Length, sequences=y_Test, padding=\"post\", value=-100)\n",
    "ts_tags = y_Test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loder objects for train dataset , validation dataset & test dataset\n",
    "\n",
    "#------------- trainloader------------------------\n",
    "x_tr = torch.tensor(train_ids, dtype=torch.long)\n",
    "y_tr = torch.tensor(tr_tags, dtype=torch.float32)\n",
    "\n",
    "\n",
    "train_data = TensorDataset(x_tr, y_tr)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "trainloader = DataLoader(train_data, sampler=train_sampler, batch_size=BatchSize)\n",
    "\n",
    "#------------- validloader------------------------\n",
    "x_val = torch.tensor(valid_ids, dtype=torch.long)\n",
    "y_val = torch.tensor(val_tags, dtype=torch.float32)\n",
    "\n",
    "\n",
    "valid_data = TensorDataset(x_val, y_val)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "validloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BatchSize)\n",
    "\n",
    "#------------- testloader------------------------\n",
    "x_test = torch.tensor(test_ids, dtype=torch.long)\n",
    "y_test = torch.tensor(ts_tags, dtype=torch.float32)\n",
    "\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "testloader = DataLoader(test_data, batch_size=BatchSize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the different layer of model\n",
    "\n",
    "bilstm= BiLSTM(\n",
    "    input_dim=n_words,\n",
    "    embedding_dim=seq_Length,\n",
    "    hidden_dim=LSTM_N,\n",
    "    output_dim=num_labels,    # output_dim=n_tags\n",
    "    lstm_layers=2,\n",
    "    emb_dropout=0.5,\n",
    "    lstm_dropout=0.1,\n",
    "    fc_dropout=0.25,\n",
    "    word_pad_idx=n_words - 1\n",
    ")\n",
    "bilstm.init_weights()\n",
    "bilstm.init_embeddings(word_pad_idx=n_words - 1)\n",
    "print(f\"The model has {bilstm.count_parameters():,} trainable parameters.\")\n",
    "print(bilstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the defined model and check to load in CPU or GPU \n",
    "\n",
    "model = bilstm\n",
    "\n",
    "if n_gpu == 1:\n",
    "    model.cuda()\n",
    "else:     \n",
    "    model.cpu()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the loss function\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(reduction='elementwise_mean',size_average=True, ignore_index=-100)  # LogSoftmax + ClassNLL Loss\n",
    "\n",
    "if n_gpu == 1:\n",
    "    loss_function = loss_function.cuda()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the optimizer function and setting its parameters\n",
    "\n",
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(\n",
    "                  optimizer_grouped_parameters,\n",
    "                  lr=LRate,\n",
    "                  eps=1e-8\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Training  process#####\n",
    "\n",
    "startA = datetime.now()\n",
    "print(\"Now Time : \",startA)\n",
    "\n",
    "\n",
    "show_confusion_after_any_epoch = True\n",
    "show_fclassification_report_after_any_epoch = True\n",
    "Digits = 4\n",
    "\n",
    "validat_loss = 0\n",
    "min_valid_loss = 1000\n",
    "df_score = pd.DataFrame()\n",
    "\n",
    "max_valid_accuracy = 0\n",
    "max_valid_F1Score = 0\n",
    "\n",
    "yLow_loss = 0.001   # for Change\n",
    "yHigh_loss = 2.0    # for Change\n",
    "yLow_accuracy = 0  # for Change\n",
    "yLow_f1_score = 0.0  # for Change\n",
    "\n",
    "Tr_f1Score,Tr_RecallScore,Tr_PreciScore,Tr_Accure =[],[],[],[]\n",
    "Va_f1Score,Va_RecallScore,Va_PreciScore,Va_Accure =[],[],[],[]\n",
    "\n",
    "min_valid_loss_epoch = 0\n",
    "max_valid_F1Score_epoch = 0\n",
    "\n",
    "epoch = 1\n",
    "Average = 'micro'   \n",
    "mode_score = 'strict' \n",
    "Flag_Traning_Countinue = True\n",
    "Epochs_Max = 200\n",
    "while Flag_Traning_Countinue == True:\n",
    "    start = datetime.now()\n",
    "    print (\"Epoch :\", epoch,\"                             \")\n",
    "    \n",
    "    #------------ train part ------------------------\n",
    "    train_loss, valid_loss = [], []\n",
    "    predictions = []\n",
    "    true_labels = []  \n",
    "    data2 =[]\n",
    "    ii = 0\n",
    "    model.train()\n",
    "    for data,target in trainloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        target = torch.tensor(target, dtype=torch.long)\n",
    "        \n",
    "        if n_gpu == 1:\n",
    "            data = data.to('cuda')\n",
    "            target = target.to('cuda')  \n",
    "            \n",
    "        output= model(data)\n",
    "        loss = loss_function(output.flatten(start_dim=0, end_dim=1),target.flatten())\n",
    "        \n",
    "        ##  backward propagation\n",
    "        loss.backward()\n",
    "\n",
    "        ##  weight optimization\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        label_ids = target.to('cpu').numpy()\n",
    "        true_labels.extend(label_ids)\n",
    "        output2 = output.detach().cpu().numpy()\n",
    "        \n",
    "        data_tmp = data.to('cpu').numpy()\n",
    "        data2.extend(data_tmp)\n",
    "\n",
    "        predictions.extend([list(p) for p in np.argmax(output2, axis=2)]) \n",
    "        ii +=1\n",
    "        print(\"train batch:  %.f / %.f \" %(ii,len(train_ids)/BatchSize),end='\\r')\n",
    "    print(\"                                \",end='\\r')\n",
    "    training_loss = np.mean(train_loss)\n",
    "    \n",
    "    train_pred_tags = [[tags_name[l_i] for ii,l_i in enumerate(l)  if data2[i][ii] != PAD_value] for i,l in enumerate(predictions)]\n",
    "    train_tags = [[tags_name[l_i] for ii,l_i in enumerate(l)  if data2[i][ii] != PAD_value] for i,l in enumerate(true_labels)]\n",
    "    \n",
    "    Train_Acc = accuracy_score(train_tags, train_pred_tags)*100.0\n",
    "    train_F1Score = f1_score(train_tags, train_pred_tags,average=Average,scheme=IOB2, mode= mode_score )\n",
    "    train_precision_score = precision_score(train_tags, train_pred_tags,average=Average,scheme=IOB2, mode= mode_score)\n",
    "    train_recall_score = recall_score(train_tags, train_pred_tags,average=Average,scheme=IOB2, mode= mode_score)\n",
    "\n",
    "    ## --------------------evaluation part -------------------------\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    ii = 0\n",
    "    data2 =[]\n",
    "    for data, target in validloader:\n",
    "        target = torch.tensor(target, dtype=torch.long)\n",
    "        if n_gpu == 1:\n",
    "            data = data.to('cuda')\n",
    "            target = target.to('cuda')\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            output= model(data)\n",
    "\n",
    "        loss = loss_function(output.flatten(start_dim=0, end_dim=1),target.flatten())\n",
    "        \n",
    "        \n",
    "        valid_loss.append(loss.item())\n",
    "        \n",
    "        label_ids = target.to('cpu').numpy()\n",
    "        true_labels.extend(label_ids)\n",
    "        output2 = output.detach().cpu().numpy()\n",
    "        \n",
    "        data_tmp = data.to('cpu').numpy()\n",
    "        data2.extend(data_tmp)\n",
    "        \n",
    "        predictions.extend([list(p) for p in np.argmax(output2, axis=2)]) \n",
    "        ii += 1\n",
    "        print(\"valid batch:  %.f / %.f \" %(ii,len(valid_ids)/BatchSize),end='\\r')\n",
    "\n",
    "\n",
    "    validat_loss = np.mean(valid_loss) \n",
    "    \n",
    "    valid_pred_tags = [[tags_name[l_i] for ii,l_i in enumerate(l)  if data2[i][ii] != PAD_value] for i,l in enumerate(predictions)]\n",
    "    valid_tags = [[tags_name[l_i] for ii,l_i in enumerate(l)  if data2[i][ii] != PAD_value] for i,l in enumerate(true_labels)]\n",
    "\n",
    "    \n",
    "    Valid_Acc = accuracy_score(valid_tags,valid_pred_tags)*100.0\n",
    "    valid_F1Score = f1_score(valid_tags,valid_pred_tags,average=Average,scheme=IOB2, mode= mode_score )\n",
    "    valid_precision_score = precision_score(valid_tags,valid_pred_tags,average=Average,scheme=IOB2, mode= mode_score)\n",
    "    valid_recall_score = recall_score(valid_tags,valid_pred_tags,average=Average,scheme=IOB2, mode= mode_score)\n",
    "    \n",
    "    #----------- Svae model in min Loss Validation------------------\n",
    "    \n",
    "    if validat_loss < min_valid_loss :\n",
    "        torch.save(model,'./model/Valid_Min_Loss_Model.pt')\n",
    "        min_valid_loss = validat_loss\n",
    "        min_valid_loss_epoch = epoch\n",
    "    \n",
    "    # ------------------- Report & Graph -------------------------\n",
    "    if epoch == 1:\n",
    "        temp = pd.DataFrame({\n",
    "                             'Tr_accur':[0],\n",
    "                             'Va_accur':[0],\n",
    "                             'Tr_loss' : [0],\n",
    "                             'Va_loss' : [0],\n",
    "                             'Tr_F1':[0],\n",
    "                             'Va_F1':[0],\n",
    "                             'Tr_precis':[0],  \n",
    "                             'Va_precis':[0],\n",
    "                             'Tr_recall':[0],  \n",
    "                             'Va_recall':[0]\n",
    "                             }) \n",
    "        df_score = df_score.append(temp, ignore_index = True) \n",
    "    temp = pd.DataFrame({\n",
    "                             'Tr_accur':[Train_Acc],\n",
    "                             'Va_accur':[Valid_Acc],\n",
    "                             'Tr_loss' : [training_loss],\n",
    "                             'Va_loss' : [validat_loss],\n",
    "                             'Tr_F1':[train_F1Score],  \n",
    "                             'Va_F1':[valid_F1Score],\n",
    "                             'Tr_precis':[train_precision_score],  \n",
    "                             'Va_precis':[valid_precision_score],\n",
    "                             'Tr_recall':[train_recall_score],  \n",
    "                             'Va_recall':[valid_recall_score]\n",
    "                             })    \n",
    "    df_score = df_score.append(temp, ignore_index = True)\n",
    "     \n",
    "    print (Fore.BLUE +\"            Accuracy |     Loss    |   Precision |    Recall   |   F1-score  |\"+Fore.RESET)\n",
    "    print('Training  :  %.2f %% |    %.4f   |    %.4f   |    %.4f   |    %.4f   |'\n",
    "           %(Train_Acc,training_loss,train_precision_score,train_recall_score,train_F1Score))\n",
    "    print('Validation:  %.2f %% |    %.4f   |    %.4f   |    %.4f   |    %.4f   |'\n",
    "           %(Valid_Acc,validat_loss,valid_precision_score,valid_recall_score,valid_F1Score))\n",
    "\n",
    "\n",
    "    plot_loss_accuracy(df_score,0, Epochs_Max, yLow_loss,yHigh_loss, yLow_accuracy, 1)\n",
    "    plot_F1_Score(df_score,0, Epochs_Max, yLow_f1_score, 1)\n",
    "      \n",
    "    if show_confusion_after_any_epoch :\n",
    "        print('\\nTraining------------------------------------------------------------------------------------')\n",
    "        Make_Confusion_Graph_Tabel(sum(train_tags,[]), sum(train_pred_tags,[]), Labels= TagLabel, Font_Scale = 1.4,\n",
    "                                                                    Prefix = True, ShowPercent= True, Sum_O_Zero= True)\n",
    "        print('\\n\\nValidation----------------------------------------------------------------------------------')\n",
    "        Make_Confusion_Graph_Tabel(sum(valid_tags,[]), sum(valid_pred_tags,[]), Labels= TagLabel, Font_Scale = 1.4,\n",
    "                                                                    Prefix = True, ShowPercent= True, Sum_O_Zero= True)\n",
    "        print('--------------------------------------------------------------------------------------------')\n",
    "        \n",
    "    if show_fclassification_report_after_any_epoch:\n",
    "        print('\\nTraining------------------------------------------------------------------------------------')\n",
    "        print(classification_report(train_tags, train_pred_tags, digits = Digits,scheme=IOB2, mode= mode_score))\n",
    "\n",
    "        print('\\n\\nValidation----------------------------------------------------------------------------------')\n",
    "        print(classification_report(valid_tags,valid_pred_tags, digits = Digits,scheme=IOB2, mode= mode_score))\n",
    "        print('-----------------------------------------------------------------------------------------------')\n",
    "       \n",
    "    end = datetime.now()\n",
    "    print(\"\")\n",
    "    print(\"Time : \", (end - start))\n",
    "    \n",
    "    print(\"________________________________________________________________________________________________________\")\n",
    "    \n",
    "#------------------------- Trainning Countine Condition Check ---------------------\n",
    "    if epoch - min_valid_loss_epoch > 10 :\n",
    "        Flag_Traning_Countinue = False\n",
    "        Epochs_Max = epoch\n",
    "    else:    \n",
    "        epoch += 1             \n",
    "#----------------------------------------------------------------------------------        \n",
    "plot_loss_accuracy(df_score,0, Epochs_Max, yLow_loss,yHigh_loss, yLow_accuracy, 1)\n",
    "plot_F1_Score(df_score,0, Epochs_Max, yLow_f1_score, 1)        \n",
    "\n",
    "\n",
    "print(\"Sum Times : \", (end - startA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testting Model (with Test Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# defining the test function using the model that saved in minimum validation loss and \n",
    "# applying it to the test dataset for prediction\n",
    "\n",
    "def select_model(model):\n",
    "    if n_gpu == 1:\n",
    "        model.cuda()\n",
    "    else:     \n",
    "        model.cpu()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    data2 =[]\n",
    "    true_labels_rep = []\n",
    "    test_loss = []\n",
    "    jj = 0\n",
    "    allPred =[]\n",
    "    outputs_test = []\n",
    "    \n",
    "    model.eval()\n",
    "    for data, target in testloader:\n",
    "    \n",
    "        target = torch.tensor(target, dtype=torch.long)\n",
    "        if n_gpu == 1:\n",
    "            data = data.to('cuda')\n",
    "            target = target.to('cuda')  \n",
    "        with torch.no_grad():\n",
    "            output= model(data)\n",
    "        loss = loss_function(output.flatten(start_dim=0, end_dim=1),target.flatten())\n",
    "        \n",
    "        test_loss.append(loss.item())\n",
    "    \n",
    "\n",
    "        label_ids = target.to('cpu').numpy()\n",
    "\n",
    "        true_labels.extend(label_ids)\n",
    "        true_labels_rep.append(label_ids)\n",
    "        \n",
    "        output2 = output.detach().cpu().numpy()\n",
    "        \n",
    "        data_tmp = data.to('cpu').numpy()\n",
    "        data2.extend(data_tmp)\n",
    "        \n",
    "        predictions.extend([list(p) for p in np.argmax(output2, axis=2)])\n",
    "        \n",
    "        outputs_test.extend(output.detach().cpu().tolist())\n",
    "    \n",
    "        jj += 1\n",
    "        print(\"Test batch:  %.f / %.f \" %(jj,len(x_test)/BatchSize),end='\\r')\n",
    "        \n",
    "        \n",
    "    outputs_test = torch.tensor(outputs_test, dtype=torch.float32).sigmoid()\n",
    "    outputs_test = np.array(outputs_test)    \n",
    "  \n",
    "    Test_Loss= np.mean(test_loss)\n",
    "\n",
    "    test_pred_tags = [[tags_name[l_i] for ii,l_i in enumerate(l)  if data2[i][ii] != PAD_value] for i,l in enumerate(predictions)]\n",
    "    test_tags = [[tags_name[l_i] for ii,l_i in enumerate(l)  if data2[i][ii] != PAD_value] for i,l in enumerate(true_labels)]\n",
    "\n",
    "    Test_Acc = accuracy_score(test_tags,test_pred_tags)*100.0\n",
    "    test_F1Score = f1_score(test_tags,test_pred_tags,average=Average,scheme=IOB2, mode= mode_score )\n",
    "    test_precision_score = precision_score(test_tags,test_pred_tags,average=Average,scheme=IOB2, mode= mode_score)\n",
    "    test_recall_score = recall_score(test_tags,test_pred_tags,average=Average,scheme=IOB2, mode= mode_score)\n",
    "\n",
    "    print (Fore.BLUE +\"       Accuracy |     Loss    |   Precision |    Recall   |   F1-score  |\"+Fore.RESET)\n",
    "    print('Test :  %.2f %% |    %.4f   |    %.4f   |    %.4f   |    %.4f   |'\n",
    "       %(Test_Acc,Test_Loss,test_precision_score,test_recall_score,test_F1Score))\n",
    "    return test_tags,test_pred_tags,outputs_test\n",
    "\n",
    "model.cpu()\n",
    "model=torch.load('./model/Valid_Min_Loss_Model.pt')\n",
    "print('Test Model Parameter: Valid Minimum Loss (epoch=',min_valid_loss_epoch,')\\n')\n",
    "test_tags,test_pred_tags,outputs = select_model(model)\n",
    "print(\"=========================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test dataset all reports (classification report, confusion matrix, and some predicted reviews sample)\n",
    "\n",
    "print('\\n*** Test Model Parameter: Valid Minimum Loss (epoch=',min_valid_loss_epoch, ') ***\\n')\n",
    "\n",
    "test_inputs_word=[]\n",
    "test_inputs_Id=[]\n",
    "test_inputs_Po=[]\n",
    "test_inputs_Se=[]\n",
    "for i in range(len(test_tags)):\n",
    "    test_words=[]\n",
    "    test_Id=[]\n",
    "    test_Po=[]\n",
    "    test_Se=[]\n",
    "    for j in range(len(test_getter[i])):\n",
    "        test_words.append(test_getter[i][j][0])\n",
    "        if j == 0 :\n",
    "            test_Id.append(test_getter[i][j][2])\n",
    "            test_Po.append(test_getter[i][j][3])\n",
    "            test_Se.append(test_getter[i][j][4])\n",
    "        else:\n",
    "            test_Id.append('')\n",
    "            test_Po.append('')\n",
    "            test_Se.append('')\n",
    "    test_inputs_word.append(test_words)\n",
    "    test_inputs_Id.append(test_Id)\n",
    "    test_inputs_Po.append(test_Po)\n",
    "    test_inputs_Se.append(test_Se)\n",
    "    \n",
    "\n",
    "words =[]\n",
    "TestTags =[]\n",
    "TestPreds =[]\n",
    "Id =[]\n",
    "Po =[]\n",
    "Se =[]\n",
    "\n",
    "for i in range(len(test_tags)):  \n",
    "\n",
    "    for aa,bb,cc,dd,ee,ff in zip(test_inputs_word[i],test_tags[i],test_pred_tags[i],test_inputs_Id[i],\n",
    "                                test_inputs_Po[i],test_inputs_Se[i]):\n",
    "        words.extend([aa])\n",
    "        TestTags.extend([bb])\n",
    "        TestPreds.extend([cc])\n",
    "        Id.extend([dd])\n",
    "        Po.extend([ee])\n",
    "        Se.extend([ff])   \n",
    "        \n",
    "check = []\n",
    "check_true = 0\n",
    "check_false = 0\n",
    "for clas,pred in zip(TestTags,TestPreds):\n",
    "    if clas == pred:\n",
    "        check.extend(['True'])\n",
    "        check_true += 1\n",
    "    else:\n",
    "        check.extend(['False'])\n",
    "        check_false += 1\n",
    "             \n",
    "\n",
    "df_predict= pd.DataFrame({'Id':Id, 'Post #':Po, 'Sentence #':Se,'Word':words,'Tag':TestTags,'Predict': TestPreds,'Check':check})\n",
    "\n",
    "df_predict.to_csv(\"./Result/Test_Dataset_Name_Entities_LSTM.csv\",sep=\",\", index=None)\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Test_Acc = accuracy_score(TestTags,TestPreds)*100.0\n",
    "test_F1Score = f1_score([TestTags],[TestPreds],average=Average,scheme=IOB2, mode= mode_score )\n",
    "test_precision_score = precision_score([TestTags],[TestPreds],average=Average,scheme=IOB2, mode= mode_score)\n",
    "test_recall_score = recall_score([TestTags],[TestPreds],average=Average,scheme=IOB2, mode= mode_score)\n",
    "\n",
    "print ('===========================================================')\n",
    "print (Fore.BLUE +\"       Accuracy |  Precision  |    Recall   |   F1-score  |\"+Fore.RESET)\n",
    "print('Test :  %.2f %% |    %.4f   |    %.4f   |    %.4f   |'\n",
    "   %(Test_Acc,test_precision_score,test_recall_score,test_F1Score))\n",
    "print ('===========================================================')\n",
    "\n",
    "#Prints-----------------------------------------------------------------------------------\n",
    "from seqeval.metrics import  classification_report\n",
    "print('----------------------------------------------------------------------------')\n",
    "print('mode: strict')\n",
    "result_report = classification_report([TestTags],[TestPreds], digits = Digits,scheme=IOB2, mode= 'strict')\n",
    "print(result_report)\n",
    "print('----------------------------------------------------------------------------')\n",
    "print('library: sklearn')\n",
    "from sklearn.metrics import  classification_report\n",
    "result_report = classification_report(TestTags,TestPreds, digits = Digits,labels =TagLabel)\n",
    "print(result_report)\n",
    "from seqeval.metrics import  classification_report\n",
    "\n",
    "print('---------------------------------------------------------------------------\\n')\n",
    "\n",
    "Make_Confusion_Graph_Tabel(TestTags,TestPreds, Labels= tags_name, Font_Scale = 1.4,\n",
    "                                                                    Prefix = True, ShowPercent= True, Sum_O_Zero= True)\n",
    "Make_Confusion_Graph_Tabel(TestTags,TestPreds, Labels= tags_name, Font_Scale = 1.4,\n",
    "                                                                    Prefix = True, ShowPercent= False, Sum_O_Zero= True)\n",
    "df_predict.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERT4",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
